{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CSCI-UA 473 : Intro to Machine Learning - Final Project"
      ],
      "metadata": {
        "id": "0S5JiWl6PY9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Device Information"
      ],
      "metadata": {
        "id": "epfWAZo1Pcew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "else:\n",
        "    print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncy282CbPYq6",
        "outputId": "03175fce-28dc-4dd2-9d28-f30a18851bcc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec 14 23:19:59 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    55W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "    print('Not using a high-RAM runtime')\n",
        "else:\n",
        "    print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NJxmUVDPj43",
        "outputId": "14580ca3-b4f8-4270-f429-b8974509ee82"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 89.6 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Setup"
      ],
      "metadata": {
        "id": "rDG_tKvDPmyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Colab sample data\n",
        "!rm -rf sample_data/\n",
        "\n",
        "# Kaggle API credentials\n",
        "%env KAGGLE_USERNAME=<KAGGLE_USERNAME>\n",
        "%env KAGGLE_KEY=<KAGGLE_KEY>"
      ],
      "metadata": {
        "id": "VqV-21aDQTjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Dataset"
      ],
      "metadata": {
        "id": "Pq9YgWWCQn64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download train and test sets\n",
        "!kaggle competitions download -c csci-ua-473-intro-to-machine-learning-fall22"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbIQd-XbQqgZ",
        "outputId": "205e229e-61f5-4680-aa47-462722d4fc82"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "csci-ua-473-intro-to-machine-learning-fall22.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip data\n",
        "!unzip -n /content/csci-ua-473-intro-to-machine-learning-fall22.zip"
      ],
      "metadata": {
        "id": "ddZYPFy6SFx4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5049c916-332e-44b4-e33e-34b0dfcee8c4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/csci-ua-473-intro-to-machine-learning-fall22.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Packages"
      ],
      "metadata": {
        "id": "Iuzq8yjIW8DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler"
      ],
      "metadata": {
        "id": "xvI_B23kXQMi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "gmxvhFkSgAdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load train set\n",
        "train_X = torch.load(\"train/train/trainX.pt\")\n",
        "train_Y = torch.load(\"train/train/trainY.pt\")\n",
        "\n",
        "# Load test set\n",
        "test_X = torch.load(\"test/test/testX.pt\")"
      ],
      "metadata": {
        "id": "SP5p9lLuXQ1C"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train set into train/val set\n",
        "train_size = len(train_X[2])\n",
        "val_start_idx = int(train_size * 0.7)\n",
        "\n",
        "val_X = [train_X[0][val_start_idx:, :, :, :, :],\n",
        "         train_X[1][val_start_idx:, :, :, :],\n",
        "         train_X[2][val_start_idx:]]\n",
        "\n",
        "val_Y = [train_Y[0][val_start_idx:, :]]\n",
        "\n",
        "train_X = [train_X[0][:val_start_idx, :, :, :, :],\n",
        "           train_X[1][:val_start_idx, :, :, :],\n",
        "           train_X[2][:val_start_idx]]\n",
        "\n",
        "train_Y = [train_Y[0][:val_start_idx, :]]"
      ],
      "metadata": {
        "id": "TxiMTTg4Bepc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Profile train/test data\n",
        "print(\"----- Total Train Size:\", train_size, \"-----\\n\")\n",
        "\n",
        "print(\"Length of train_X:\", len(train_X))\n",
        "print(\"Shape of train_X rgb_images:\", train_X[0].shape)\n",
        "print(\"Shape of train_X depth_images:\", train_X[1].shape)\n",
        "print(\"Length of train_X file_ids:\", len(train_X[2]), \"\\n\")\n",
        "\n",
        "print(\"Shape of train_Y:\", train_Y[0].shape, \"\\n\")\n",
        "\n",
        "print(\"Length of val_X:\", len(val_X))\n",
        "print(\"Shape of val_X rgb_images:\", val_X[0].shape)\n",
        "print(\"Shape of val_X depth_images:\", val_X[1].shape)\n",
        "print(\"Length of val_X file_ids:\", len(val_X[2]), \"\\n\")\n",
        "\n",
        "print(\"Shape of val_Y:\", val_Y[0].shape, \"\\n\")\n",
        "\n",
        "print(\"----- Total Test Size:\", len(test_X[2]), \"-----\\n\")\n",
        "\n",
        "print(\"Length of test_X:\", len(test_X))\n",
        "print(\"Shape of test_X rgb_images:\", test_X[0].shape)\n",
        "print(\"Shape of test_X depth_images:\", test_X[1].shape)\n",
        "print(\"Length of test_X file_ids:\", len(test_X[2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0Z_hQoiappx",
        "outputId": "617d77d0-7810-4754-bb8f-651fc8d33cc0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Total Train Size: 3396 -----\n",
            "\n",
            "Length of train_X: 3\n",
            "Shape of train_X rgb_images: torch.Size([2377, 3, 3, 224, 224])\n",
            "Shape of train_X depth_images: torch.Size([2377, 3, 224, 224])\n",
            "Length of train_X file_ids: 2377 \n",
            "\n",
            "Shape of train_Y: torch.Size([2377, 12]) \n",
            "\n",
            "Length of val_X: 3\n",
            "Shape of val_X rgb_images: torch.Size([1019, 3, 3, 224, 224])\n",
            "Shape of val_X depth_images: torch.Size([1019, 3, 224, 224])\n",
            "Length of val_X file_ids: 1019 \n",
            "\n",
            "Shape of val_Y: torch.Size([1019, 12]) \n",
            "\n",
            "----- Total Test Size: 849 -----\n",
            "\n",
            "Length of test_X: 3\n",
            "Shape of test_X rgb_images: torch.Size([849, 3, 3, 224, 224])\n",
            "Shape of test_X depth_images: torch.Size([849, 3, 224, 224])\n",
            "Length of test_X file_ids: 849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Custom Dataset"
      ],
      "metadata": {
        "id": "SIWfSE0ncNJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RobotHandDataset(Dataset):\n",
        "    def __init__(self, X, Y=None, view='top', rgb_transform=None, depth_transform=None, target_transform=None):\n",
        "        self.rgb_images = X[0]\n",
        "        self.depth_images = X[1]\n",
        "        self.file_ids = X[2]\n",
        "\n",
        "        if view == 'top':\n",
        "            self.view = 0\n",
        "        elif view == 'left':\n",
        "            self.view = 1\n",
        "        else:\n",
        "            self.view = 2\n",
        "\n",
        "        self.robot_states = self.robot_states = Y[0] if Y != None else None\n",
        "\n",
        "        self.rgb_transform = rgb_transform\n",
        "        self.target_transform = target_transform\n",
        "        self.depth_transform = depth_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.rgb_images.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        rgb_image = self.rgb_images[idx][self.view]\n",
        "        depth_image = self.depth_images[idx][self.view]\n",
        "\n",
        "        if self.robot_states != None:\n",
        "            state = self.robot_states[idx]\n",
        "        \n",
        "        if self.depth_transform:\n",
        "            depth_image = self.depth_transform(depth_image)\n",
        "        \n",
        "        if self.rgb_transform:\n",
        "            rgb_image = self.rgb_transform(x=rgb_image, depth_image=depth_image)\n",
        "\n",
        "        if self.robot_states != None and self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        \n",
        "        rgbd_image = torch.cat((rgb_image, depth_image), 0)\n",
        "\n",
        "        # rgbd_image = rgbd_image.permute(1, 2, 0)\n",
        "\n",
        "        return rgbd_image.float(), state.float()"
      ],
      "metadata": {
        "id": "G3ZIhV_CcOKU"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Dataset and DataLoader"
      ],
      "metadata": {
        "id": "FheGw8XwgKWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Calculate normalization metrics\n",
        "VIEW = 0\n",
        "\n",
        "mean_r = np.array(train_X[0][:, VIEW, 0, :, :].flatten() / 255).mean()\n",
        "mean_g = np.array(train_X[0][:, VIEW, 1, :, :].flatten() / 255).mean()\n",
        "mean_b = np.array(train_X[0][:, VIEW, 2, :, :].flatten() / 255).mean()\n",
        "\n",
        "std_r = np.array(train_X[0][:, VIEW, 0, :, :].flatten() / 255).std()\n",
        "std_g = np.array(train_X[0][:, VIEW, 1, :, :].flatten() / 255).std()\n",
        "std_b = np.array(train_X[0][:, VIEW, 2, :, :].flatten() / 255).std()\n",
        "\n",
        "mean_depth = np.array(train_X[1][:, VIEW, :, :].flatten() / 1000).mean()\n",
        "std_depth = np.array(train_X[1][:, VIEW, :, :].flatten() / 1000).std()"
      ],
      "metadata": {
        "id": "vUbzi2fAr6ko"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rgb_transform(x, depth_image):\n",
        "    q50, q37, q25 = np.percentile(depth_image[0].flatten(), [50, 37.5, 25])\n",
        "\n",
        "    indices = np.where(depth_image[0] > q37)\n",
        "    x[0, :, :][indices] = 0\n",
        "    x[1, :, :][indices] = 0\n",
        "    x[2, :, :][indices] = 0\n",
        "\n",
        "    cube_threshold = 80.0\n",
        "    indices = np.where(x[0, :, :] < cube_threshold)\n",
        "    x[0, :, :][indices] = 0\n",
        "    x[1, :, :][indices] = 0\n",
        "    x[2, :, :][indices] = 0\n",
        "\n",
        "    indices = np.where(x[1, :, :] < cube_threshold)\n",
        "    x[0, :, :][indices] = 0\n",
        "    x[1, :, :][indices] = 0\n",
        "    x[2, :, :][indices] = 0\n",
        "\n",
        "    indices = np.where(x[2, :, :] < cube_threshold)\n",
        "    x[0, :, :][indices] = 0\n",
        "    x[1, :, :][indices] = 0\n",
        "    x[2, :, :][indices] = 0\n",
        "\n",
        "    table_threshold = 170.0\n",
        "    indices1 = np.where(x[0, :, :] < table_threshold, True, False)\n",
        "    indices2 = np.where(x[1, :, :] < table_threshold, True, False)\n",
        "    indices3 = np.where(x[2, :, :] < table_threshold, True, False)\n",
        "    \n",
        "    indices = np.all([indices1, indices2, indices3], axis=0)\n",
        "    \n",
        "    x[0, :, :][indices] = 0\n",
        "    x[1, :, :][indices] = 0\n",
        "    x[2, :, :][indices] = 0\n",
        "\n",
        "    x = x / 255.0\n",
        "    \n",
        "    # x = transforms.Normalize([mean_r, mean_g, mean_b], [std_r, std_g, std_b]).forward(x)\n",
        "    # x = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]).forward(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "def depth_transform(x):\n",
        "    x = torch.tensor(np.expand_dims(x, 0))\n",
        "    x = x / 1000.0\n",
        "    x = transforms.Normalize([mean_depth], [std_depth]).forward(x)\n",
        "    return x\n",
        "\n",
        "datasets = {\n",
        "    \"train\": RobotHandDataset(X=train_X, Y=train_Y, view='top',\n",
        "                              rgb_transform=rgb_transform, depth_transform=depth_transform),\n",
        "    \"val\": RobotHandDataset(X=val_X, Y=val_Y, view='top',\n",
        "                            rgb_transform=rgb_transform, depth_transform=depth_transform),\n",
        "    \"test\": RobotHandDataset(X=test_X, Y=val_Y, view='top',\n",
        "                             rgb_transform=rgb_transform, depth_transform=depth_transform)\n",
        "}\n",
        "\n",
        "dataloaders = {\n",
        "    'train': DataLoader(datasets['train'], batch_size=32, shuffle=True, num_workers=2),\n",
        "    'val': DataLoader(datasets['val'], batch_size=32, shuffle=True, num_workers=2),\n",
        "    'test': DataLoader(datasets['test'], batch_size=1, shuffle=False, num_workers=2),\n",
        "}\n",
        "\n",
        "dataset_sizes = {x: len(datasets[x]) for x in ['train', 'val', 'test']}\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Current device:\", device)"
      ],
      "metadata": {
        "id": "k2JOoH_PfRis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c853c34-8937-4e8f-88cc-fa7a2f24bf1f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display Images"
      ],
      "metadata": {
        "id": "d-FaoawZsoAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display image and label.\n",
        "train_rgbd_images, train_states = datasets[\"train\"].__getitem__(0)\n",
        "\n",
        "print(f\"Image batch shape: {train_rgbd_images.size()}\", \"\\n\")\n",
        "print(f\"Labels batch shape: {train_states.size()}\", \"\\n\")\n",
        "\n",
        "print(f\"State: {train_states[0]}\", \"\\n\")\n",
        "\n",
        "img = train_rgbd_images[:3,:,:].permute(1, 2, 0).squeeze()\n",
        "plt.imshow(img)\n",
        "# plt.gcf().set_dpi(300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H111njqHgZzA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "f9e92c4c-4aa5-491e-a8f9-1339cbcad38d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image batch shape: torch.Size([4, 224, 224]) \n",
            "\n",
            "Labels batch shape: torch.Size([12]) \n",
            "\n",
            "State: 0.026672374457120895 \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer Learning"
      ],
      "metadata": {
        "id": "s-Rf0UoY1_6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=50):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, states in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                states = states.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = torch.sqrt(criterion(outputs, states))\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                print(f\"Current LR: {scheduler.get_last_lr()[0]:.8f}\")\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.5f}')\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_loss < best_loss:\n",
        "                best_loss_unchange = 0\n",
        "                best_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            \n",
        "            if phase == 'val':\n",
        "                print(f\"Best Loss: {best_loss:.5f}\")\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val loss: {best_loss:.5f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "metadata": {
        "id": "qaCUDNWK7e61"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet50(weights=\"IMAGENET1K_V2\")\n",
        "\n",
        "# Change num input channels from 3 to 4\n",
        "weight = model.conv1.weight.data.clone()\n",
        "\n",
        "model.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.conv1.weight.data[:, :3] = weight\n",
        "    model.conv1.weight.data[:, 3] = model.conv1.weight.data[:, 0]\n",
        "\n",
        "num_ftrs = model.fc.in_features\n",
        "\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model.fc = nn.Linear(num_ftrs, train_states.size(0))\n",
        "\n",
        "model_ft = model.to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=50, gamma=0.1)"
      ],
      "metadata": {
        "id": "ba2RcR_l2FeP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jtF0VBQsNSz",
        "outputId": "eb65bc36-9aa4-4b7d-f3d9-cf32a2339e36"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=12, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=300)"
      ],
      "metadata": {
        "id": "UIFB-Qg4QwgD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c096801-9f72-478b-8b49-586911eac6f1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.02554\n",
            "val Loss: 0.01230\n",
            "Best Loss: 0.01230\n",
            "\n",
            "Epoch 2/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00996\n",
            "val Loss: 0.01082\n",
            "Best Loss: 0.01082\n",
            "\n",
            "Epoch 3/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00851\n",
            "val Loss: 0.00860\n",
            "Best Loss: 0.00860\n",
            "\n",
            "Epoch 4/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00740\n",
            "val Loss: 0.00811\n",
            "Best Loss: 0.00811\n",
            "\n",
            "Epoch 5/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00716\n",
            "val Loss: 0.00719\n",
            "Best Loss: 0.00719\n",
            "\n",
            "Epoch 6/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00616\n",
            "val Loss: 0.00661\n",
            "Best Loss: 0.00661\n",
            "\n",
            "Epoch 7/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00622\n",
            "val Loss: 0.00612\n",
            "Best Loss: 0.00612\n",
            "\n",
            "Epoch 8/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00578\n",
            "val Loss: 0.00601\n",
            "Best Loss: 0.00601\n",
            "\n",
            "Epoch 9/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00591\n",
            "val Loss: 0.00751\n",
            "Best Loss: 0.00601\n",
            "\n",
            "Epoch 10/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00593\n",
            "val Loss: 0.00803\n",
            "Best Loss: 0.00601\n",
            "\n",
            "Epoch 11/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00528\n",
            "val Loss: 0.00542\n",
            "Best Loss: 0.00542\n",
            "\n",
            "Epoch 12/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00532\n",
            "val Loss: 0.00536\n",
            "Best Loss: 0.00536\n",
            "\n",
            "Epoch 13/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00524\n",
            "val Loss: 0.00778\n",
            "Best Loss: 0.00536\n",
            "\n",
            "Epoch 14/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00490\n",
            "val Loss: 0.00651\n",
            "Best Loss: 0.00536\n",
            "\n",
            "Epoch 15/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00516\n",
            "val Loss: 0.00773\n",
            "Best Loss: 0.00536\n",
            "\n",
            "Epoch 16/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00518\n",
            "val Loss: 0.00615\n",
            "Best Loss: 0.00536\n",
            "\n",
            "Epoch 17/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00505\n",
            "val Loss: 0.00620\n",
            "Best Loss: 0.00536\n",
            "\n",
            "Epoch 18/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00523\n",
            "val Loss: 0.00612\n",
            "Best Loss: 0.00536\n",
            "\n",
            "Epoch 19/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00494\n",
            "val Loss: 0.00571\n",
            "Best Loss: 0.00536\n",
            "\n",
            "Epoch 20/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00503\n",
            "val Loss: 0.00465\n",
            "Best Loss: 0.00465\n",
            "\n",
            "Epoch 21/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00496\n",
            "val Loss: 0.00548\n",
            "Best Loss: 0.00465\n",
            "\n",
            "Epoch 22/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00520\n",
            "val Loss: 0.00692\n",
            "Best Loss: 0.00465\n",
            "\n",
            "Epoch 23/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00481\n",
            "val Loss: 0.00533\n",
            "Best Loss: 0.00465\n",
            "\n",
            "Epoch 24/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00479\n",
            "val Loss: 0.00544\n",
            "Best Loss: 0.00465\n",
            "\n",
            "Epoch 25/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00503\n",
            "val Loss: 0.00622\n",
            "Best Loss: 0.00465\n",
            "\n",
            "Epoch 26/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00498\n",
            "val Loss: 0.00549\n",
            "Best Loss: 0.00465\n",
            "\n",
            "Epoch 27/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00470\n",
            "val Loss: 0.00514\n",
            "Best Loss: 0.00465\n",
            "\n",
            "Epoch 28/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00477\n",
            "val Loss: 0.00519\n",
            "Best Loss: 0.00465\n",
            "\n",
            "Epoch 29/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00465\n",
            "val Loss: 0.00518\n",
            "Best Loss: 0.00465\n",
            "\n",
            "Epoch 30/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00451\n",
            "val Loss: 0.00725\n",
            "Best Loss: 0.00465\n",
            "\n",
            "Epoch 31/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00469\n",
            "val Loss: 0.00552\n",
            "Best Loss: 0.00465\n",
            "\n",
            "Epoch 32/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00462\n",
            "val Loss: 0.00409\n",
            "Best Loss: 0.00409\n",
            "\n",
            "Epoch 33/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00476\n",
            "val Loss: 0.00695\n",
            "Best Loss: 0.00409\n",
            "\n",
            "Epoch 34/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00483\n",
            "val Loss: 0.00628\n",
            "Best Loss: 0.00409\n",
            "\n",
            "Epoch 35/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00470\n",
            "val Loss: 0.00595\n",
            "Best Loss: 0.00409\n",
            "\n",
            "Epoch 36/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00446\n",
            "val Loss: 0.00676\n",
            "Best Loss: 0.00409\n",
            "\n",
            "Epoch 37/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00468\n",
            "val Loss: 0.00606\n",
            "Best Loss: 0.00409\n",
            "\n",
            "Epoch 38/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00476\n",
            "val Loss: 0.00494\n",
            "Best Loss: 0.00409\n",
            "\n",
            "Epoch 39/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00449\n",
            "val Loss: 0.00646\n",
            "Best Loss: 0.00409\n",
            "\n",
            "Epoch 40/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00470\n",
            "val Loss: 0.00666\n",
            "Best Loss: 0.00409\n",
            "\n",
            "Epoch 41/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00475\n",
            "val Loss: 0.00611\n",
            "Best Loss: 0.00409\n",
            "\n",
            "Epoch 42/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00472\n",
            "val Loss: 0.00520\n",
            "Best Loss: 0.00409\n",
            "\n",
            "Epoch 43/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00449\n",
            "val Loss: 0.00506\n",
            "Best Loss: 0.00409\n",
            "\n",
            "Epoch 44/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00471\n",
            "val Loss: 0.00679\n",
            "Best Loss: 0.00409\n",
            "\n",
            "Epoch 45/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00441\n",
            "val Loss: 0.00459\n",
            "Best Loss: 0.00409\n",
            "\n",
            "Epoch 46/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00441\n",
            "val Loss: 0.00617\n",
            "Best Loss: 0.00409\n",
            "\n",
            "Epoch 47/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00460\n",
            "val Loss: 0.00581\n",
            "Best Loss: 0.00409\n",
            "\n",
            "Epoch 48/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00437\n",
            "val Loss: 0.00606\n",
            "Best Loss: 0.00409\n",
            "\n",
            "Epoch 49/300\n",
            "----------\n",
            "Current LR: 0.10000000\n",
            "train Loss: 0.00459\n",
            "val Loss: 0.00676\n",
            "Best Loss: 0.00409\n",
            "\n",
            "Epoch 50/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00513\n",
            "val Loss: 0.00652\n",
            "Best Loss: 0.00409\n",
            "\n",
            "Epoch 51/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00244\n",
            "val Loss: 0.00364\n",
            "Best Loss: 0.00364\n",
            "\n",
            "Epoch 52/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00194\n",
            "val Loss: 0.00354\n",
            "Best Loss: 0.00354\n",
            "\n",
            "Epoch 53/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00182\n",
            "val Loss: 0.00350\n",
            "Best Loss: 0.00350\n",
            "\n",
            "Epoch 54/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00186\n",
            "val Loss: 0.00347\n",
            "Best Loss: 0.00347\n",
            "\n",
            "Epoch 55/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00179\n",
            "val Loss: 0.00334\n",
            "Best Loss: 0.00334\n",
            "\n",
            "Epoch 56/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00179\n",
            "val Loss: 0.00334\n",
            "Best Loss: 0.00334\n",
            "\n",
            "Epoch 57/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00173\n",
            "val Loss: 0.00348\n",
            "Best Loss: 0.00334\n",
            "\n",
            "Epoch 58/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00171\n",
            "val Loss: 0.00347\n",
            "Best Loss: 0.00334\n",
            "\n",
            "Epoch 59/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00175\n",
            "val Loss: 0.00351\n",
            "Best Loss: 0.00334\n",
            "\n",
            "Epoch 60/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00165\n",
            "val Loss: 0.00337\n",
            "Best Loss: 0.00334\n",
            "\n",
            "Epoch 61/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00165\n",
            "val Loss: 0.00334\n",
            "Best Loss: 0.00334\n",
            "\n",
            "Epoch 62/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00161\n",
            "val Loss: 0.00343\n",
            "Best Loss: 0.00334\n",
            "\n",
            "Epoch 63/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00167\n",
            "val Loss: 0.00350\n",
            "Best Loss: 0.00334\n",
            "\n",
            "Epoch 64/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00164\n",
            "val Loss: 0.00336\n",
            "Best Loss: 0.00334\n",
            "\n",
            "Epoch 65/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00167\n",
            "val Loss: 0.00337\n",
            "Best Loss: 0.00334\n",
            "\n",
            "Epoch 66/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00158\n",
            "val Loss: 0.00331\n",
            "Best Loss: 0.00331\n",
            "\n",
            "Epoch 67/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00159\n",
            "val Loss: 0.00331\n",
            "Best Loss: 0.00331\n",
            "\n",
            "Epoch 68/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00160\n",
            "val Loss: 0.00344\n",
            "Best Loss: 0.00331\n",
            "\n",
            "Epoch 69/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00160\n",
            "val Loss: 0.00338\n",
            "Best Loss: 0.00331\n",
            "\n",
            "Epoch 70/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00158\n",
            "val Loss: 0.00334\n",
            "Best Loss: 0.00331\n",
            "\n",
            "Epoch 71/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00159\n",
            "val Loss: 0.00339\n",
            "Best Loss: 0.00331\n",
            "\n",
            "Epoch 72/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00155\n",
            "val Loss: 0.00337\n",
            "Best Loss: 0.00331\n",
            "\n",
            "Epoch 73/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00159\n",
            "val Loss: 0.00337\n",
            "Best Loss: 0.00331\n",
            "\n",
            "Epoch 74/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00153\n",
            "val Loss: 0.00336\n",
            "Best Loss: 0.00331\n",
            "\n",
            "Epoch 75/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00153\n",
            "val Loss: 0.00328\n",
            "Best Loss: 0.00328\n",
            "\n",
            "Epoch 76/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00153\n",
            "val Loss: 0.00334\n",
            "Best Loss: 0.00328\n",
            "\n",
            "Epoch 77/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00155\n",
            "val Loss: 0.00345\n",
            "Best Loss: 0.00328\n",
            "\n",
            "Epoch 78/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00151\n",
            "val Loss: 0.00343\n",
            "Best Loss: 0.00328\n",
            "\n",
            "Epoch 79/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00155\n",
            "val Loss: 0.00345\n",
            "Best Loss: 0.00328\n",
            "\n",
            "Epoch 80/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00151\n",
            "val Loss: 0.00350\n",
            "Best Loss: 0.00328\n",
            "\n",
            "Epoch 81/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00157\n",
            "val Loss: 0.00341\n",
            "Best Loss: 0.00328\n",
            "\n",
            "Epoch 82/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00152\n",
            "val Loss: 0.00340\n",
            "Best Loss: 0.00328\n",
            "\n",
            "Epoch 83/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00150\n",
            "val Loss: 0.00350\n",
            "Best Loss: 0.00328\n",
            "\n",
            "Epoch 84/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00155\n",
            "val Loss: 0.00338\n",
            "Best Loss: 0.00328\n",
            "\n",
            "Epoch 85/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00155\n",
            "val Loss: 0.00347\n",
            "Best Loss: 0.00328\n",
            "\n",
            "Epoch 86/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00148\n",
            "val Loss: 0.00340\n",
            "Best Loss: 0.00328\n",
            "\n",
            "Epoch 87/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00152\n",
            "val Loss: 0.00347\n",
            "Best Loss: 0.00328\n",
            "\n",
            "Epoch 88/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00148\n",
            "val Loss: 0.00329\n",
            "Best Loss: 0.00328\n",
            "\n",
            "Epoch 89/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00146\n",
            "val Loss: 0.00338\n",
            "Best Loss: 0.00328\n",
            "\n",
            "Epoch 90/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00144\n",
            "val Loss: 0.00330\n",
            "Best Loss: 0.00328\n",
            "\n",
            "Epoch 91/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00149\n",
            "val Loss: 0.00330\n",
            "Best Loss: 0.00328\n",
            "\n",
            "Epoch 92/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00147\n",
            "val Loss: 0.00335\n",
            "Best Loss: 0.00328\n",
            "\n",
            "Epoch 93/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00142\n",
            "val Loss: 0.00338\n",
            "Best Loss: 0.00328\n",
            "\n",
            "Epoch 94/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00150\n",
            "val Loss: 0.00339\n",
            "Best Loss: 0.00328\n",
            "\n",
            "Epoch 95/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00144\n",
            "val Loss: 0.00331\n",
            "Best Loss: 0.00328\n",
            "\n",
            "Epoch 96/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00142\n",
            "val Loss: 0.00335\n",
            "Best Loss: 0.00328\n",
            "\n",
            "Epoch 97/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00142\n",
            "val Loss: 0.00333\n",
            "Best Loss: 0.00328\n",
            "\n",
            "Epoch 98/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00143\n",
            "val Loss: 0.00332\n",
            "Best Loss: 0.00328\n",
            "\n",
            "Epoch 99/300\n",
            "----------\n",
            "Current LR: 0.01000000\n",
            "train Loss: 0.00146\n",
            "val Loss: 0.00342\n",
            "Best Loss: 0.00328\n",
            "\n",
            "Epoch 100/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00147\n",
            "val Loss: 0.00339\n",
            "Best Loss: 0.00328\n",
            "\n",
            "Epoch 101/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00129\n",
            "val Loss: 0.00327\n",
            "Best Loss: 0.00327\n",
            "\n",
            "Epoch 102/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00127\n",
            "val Loss: 0.00329\n",
            "Best Loss: 0.00327\n",
            "\n",
            "Epoch 103/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00127\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00324\n",
            "\n",
            "Epoch 104/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00125\n",
            "val Loss: 0.00323\n",
            "Best Loss: 0.00323\n",
            "\n",
            "Epoch 105/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00124\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00323\n",
            "\n",
            "Epoch 106/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00127\n",
            "val Loss: 0.00335\n",
            "Best Loss: 0.00323\n",
            "\n",
            "Epoch 107/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00124\n",
            "val Loss: 0.00326\n",
            "Best Loss: 0.00323\n",
            "\n",
            "Epoch 108/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00123\n",
            "val Loss: 0.00326\n",
            "Best Loss: 0.00323\n",
            "\n",
            "Epoch 109/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00125\n",
            "val Loss: 0.00329\n",
            "Best Loss: 0.00323\n",
            "\n",
            "Epoch 110/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00123\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00323\n",
            "\n",
            "Epoch 111/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00125\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00323\n",
            "\n",
            "Epoch 112/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00126\n",
            "val Loss: 0.00325\n",
            "Best Loss: 0.00323\n",
            "\n",
            "Epoch 113/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00125\n",
            "val Loss: 0.00323\n",
            "Best Loss: 0.00323\n",
            "\n",
            "Epoch 114/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00122\n",
            "val Loss: 0.00325\n",
            "Best Loss: 0.00323\n",
            "\n",
            "Epoch 115/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00125\n",
            "val Loss: 0.00323\n",
            "Best Loss: 0.00323\n",
            "\n",
            "Epoch 116/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00126\n",
            "val Loss: 0.00332\n",
            "Best Loss: 0.00323\n",
            "\n",
            "Epoch 117/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00124\n",
            "val Loss: 0.00331\n",
            "Best Loss: 0.00323\n",
            "\n",
            "Epoch 118/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00127\n",
            "val Loss: 0.00326\n",
            "Best Loss: 0.00323\n",
            "\n",
            "Epoch 119/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00122\n",
            "val Loss: 0.00325\n",
            "Best Loss: 0.00323\n",
            "\n",
            "Epoch 120/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00124\n",
            "val Loss: 0.00322\n",
            "Best Loss: 0.00322\n",
            "\n",
            "Epoch 121/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00125\n",
            "val Loss: 0.00326\n",
            "Best Loss: 0.00322\n",
            "\n",
            "Epoch 122/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00123\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00322\n",
            "\n",
            "Epoch 123/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00122\n",
            "val Loss: 0.00330\n",
            "Best Loss: 0.00322\n",
            "\n",
            "Epoch 124/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00125\n",
            "val Loss: 0.00329\n",
            "Best Loss: 0.00322\n",
            "\n",
            "Epoch 125/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00122\n",
            "val Loss: 0.00322\n",
            "Best Loss: 0.00322\n",
            "\n",
            "Epoch 126/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00323\n",
            "Best Loss: 0.00322\n",
            "\n",
            "Epoch 127/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00327\n",
            "Best Loss: 0.00322\n",
            "\n",
            "Epoch 128/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00328\n",
            "Best Loss: 0.00322\n",
            "\n",
            "Epoch 129/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00123\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00322\n",
            "\n",
            "Epoch 130/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00322\n",
            "\n",
            "Epoch 131/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00124\n",
            "val Loss: 0.00323\n",
            "Best Loss: 0.00322\n",
            "\n",
            "Epoch 132/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00322\n",
            "\n",
            "Epoch 133/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00124\n",
            "val Loss: 0.00325\n",
            "Best Loss: 0.00322\n",
            "\n",
            "Epoch 134/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00124\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00322\n",
            "\n",
            "Epoch 135/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00122\n",
            "val Loss: 0.00326\n",
            "Best Loss: 0.00322\n",
            "\n",
            "Epoch 136/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00123\n",
            "val Loss: 0.00326\n",
            "Best Loss: 0.00322\n",
            "\n",
            "Epoch 137/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00123\n",
            "val Loss: 0.00326\n",
            "Best Loss: 0.00322\n",
            "\n",
            "Epoch 138/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00323\n",
            "Best Loss: 0.00322\n",
            "\n",
            "Epoch 139/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00321\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 140/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00123\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 141/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00122\n",
            "val Loss: 0.00322\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 142/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00126\n",
            "val Loss: 0.00330\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 143/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00122\n",
            "val Loss: 0.00325\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 144/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00123\n",
            "val Loss: 0.00322\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 145/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00325\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 146/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00327\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 147/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00122\n",
            "val Loss: 0.00322\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 148/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00122\n",
            "val Loss: 0.00323\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 149/300\n",
            "----------\n",
            "Current LR: 0.00100000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00323\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 150/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00122\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 151/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00122\n",
            "val Loss: 0.00328\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 152/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00325\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 153/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00328\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 154/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00122\n",
            "val Loss: 0.00328\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 155/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 156/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00327\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 157/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00323\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 158/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00329\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 159/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 160/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00329\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 161/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00323\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 162/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00325\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 163/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00325\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 164/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00321\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 165/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00322\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 166/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00326\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 167/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00122\n",
            "val Loss: 0.00328\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 168/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00326\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 169/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00326\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 170/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00323\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 171/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 172/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 173/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00323\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 174/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00122\n",
            "val Loss: 0.00322\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 175/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00328\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 176/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00329\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 177/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00327\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 178/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00323\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 179/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00328\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 180/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00327\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 181/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 182/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00326\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 183/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00329\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 184/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00323\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 185/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00123\n",
            "val Loss: 0.00331\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 186/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 187/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00123\n",
            "val Loss: 0.00323\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 188/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 189/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00327\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 190/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 191/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 192/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00123\n",
            "val Loss: 0.00325\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 193/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00325\n",
            "Best Loss: 0.00321\n",
            "\n",
            "Epoch 194/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00117\n",
            "val Loss: 0.00320\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 195/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00325\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 196/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00322\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 197/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00117\n",
            "val Loss: 0.00321\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 198/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00326\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 199/300\n",
            "----------\n",
            "Current LR: 0.00010000\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00321\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 200/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00322\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 201/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00122\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 202/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00327\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 203/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00323\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 204/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00328\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 205/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00323\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 206/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00321\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 207/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 208/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 209/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00327\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 210/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00325\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 211/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00327\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 212/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00327\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 213/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00329\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 214/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00325\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 215/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00322\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 216/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 217/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00321\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 218/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00326\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 219/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00117\n",
            "val Loss: 0.00321\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 220/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 221/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00122\n",
            "val Loss: 0.00335\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 222/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00326\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 223/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 224/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00327\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 225/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00326\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 226/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 227/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00322\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 228/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 229/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00329\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 230/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00122\n",
            "val Loss: 0.00323\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 231/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00329\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 232/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00321\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 233/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 234/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 235/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00323\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 236/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00323\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 237/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00328\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 238/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00326\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 239/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 240/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00326\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 241/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00323\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 242/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00117\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 243/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00322\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 244/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00329\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 245/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00322\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 246/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00327\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 247/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00331\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 248/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00322\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 249/300\n",
            "----------\n",
            "Current LR: 0.00001000\n",
            "train Loss: 0.00122\n",
            "val Loss: 0.00325\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 250/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00329\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 251/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00122\n",
            "val Loss: 0.00327\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 252/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00322\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 253/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00327\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 254/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00122\n",
            "val Loss: 0.00322\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 255/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00328\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 256/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 257/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00117\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 258/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00328\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 259/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00327\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 260/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00328\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 261/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 262/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 263/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 264/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00326\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 265/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00328\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 266/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00326\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 267/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 268/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 269/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00321\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 270/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00321\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 271/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 272/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00321\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 273/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 274/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00322\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 275/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00323\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 276/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00327\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 277/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00122\n",
            "val Loss: 0.00322\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 278/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00323\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 279/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00323\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 280/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00322\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 281/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00326\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 282/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00328\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 283/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00325\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 284/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00325\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 285/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00327\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 286/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 287/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00327\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 288/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00323\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 289/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00121\n",
            "val Loss: 0.00329\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 290/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00326\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 291/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00325\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 292/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00327\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 293/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00327\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 294/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00326\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 295/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00328\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 296/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00120\n",
            "val Loss: 0.00325\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 297/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00118\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 298/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00117\n",
            "val Loss: 0.00323\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 299/300\n",
            "----------\n",
            "Current LR: 0.00000100\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00325\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Epoch 300/300\n",
            "----------\n",
            "Current LR: 0.00000010\n",
            "train Loss: 0.00119\n",
            "val Loss: 0.00324\n",
            "Best Loss: 0.00320\n",
            "\n",
            "Training complete in 30m 15s\n",
            "Best val loss: 0.00320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try Adding Dropout Layer"
      ],
      "metadata": {
        "id": "QCLbGYOsv9U2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_model(model):\n",
        "    for child_name, child in model.named_children():\n",
        "        if isinstance(child, nn.BatchNorm2d):\n",
        "            setattr(model, child_name, nn.Sequential(\n",
        "                child,\n",
        "                nn.Dropout(p=0.1)\n",
        "            ))\n",
        "        else:\n",
        "            adjust_model(child)"
      ],
      "metadata": {
        "id": "R9mKmA1twBrw"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dropout = resnet50(weights=\"IMAGENET1K_V2\")\n",
        "\n",
        "# Change num input channels from 3 to 4\n",
        "weight = model_dropout.conv1.weight.data.clone()\n",
        "\n",
        "model_dropout.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "    model_dropout.conv1.weight.data[:, :3] = weight\n",
        "    model_dropout.conv1.weight.data[:, 3] = model_dropout.conv1.weight.data[:, 0]\n",
        "\n",
        "num_ftrs_dropout = model_dropout.fc.in_features\n",
        "\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model_dropout.fc = nn.Linear(num_ftrs_dropout, train_states.size(0))\n",
        "\n",
        "# Insert dropout layer after every BatchNorm2d layer\n",
        "adjust_model(model_dropout)\n",
        "\n",
        "model_dropout_ft = model_dropout.to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_dropout.parameters(), lr=0.1, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.1)"
      ],
      "metadata": {
        "id": "JrPiNJT4weUu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dropout_ft"
      ],
      "metadata": {
        "id": "GZZjbm031Rz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dropout_ft = train_model(model_dropout_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=50)"
      ],
      "metadata": {
        "id": "uaykzAFixR9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Output"
      ],
      "metadata": {
        "id": "bsHLjJmIZPZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf submission.csv"
      ],
      "metadata": {
        "id": "ANdk4GaZhmgD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outfile = 'submission.csv'\n",
        "\n",
        "output_file = open(outfile, 'w')\n",
        "\n",
        "titles = ['ID', 'FINGER_POS_1', 'FINGER_POS_2', 'FINGER_POS_3', 'FINGER_POS_4', 'FINGER_POS_5', 'FINGER_POS_6',\n",
        "         'FINGER_POS_7', 'FINGER_POS_8', 'FINGER_POS_9', 'FINGER_POS_10', 'FINGER_POS_11', 'FINGER_POS_12']\n",
        "\n",
        "preds = []"
      ],
      "metadata": {
        "id": "lS_R3Gk9aCm2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = torch.load('test/test/testX.pt')\n",
        "file_ids = test_data[-1]\n",
        "\n",
        "model_ft.eval()\n",
        "\n",
        "for inputs, labels in dataloaders['test']:\n",
        "    inputs = inputs.to(device)\n",
        "    outputs = model(inputs)\n",
        "    preds.append(outputs[0].cpu().detach().numpy())"
      ],
      "metadata": {
        "id": "6Lh81SyEZTkt"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([pd.DataFrame(file_ids), pd.DataFrame.from_records(preds)], axis = 1, names = titles)\n",
        "df.columns = titles\n",
        "df.to_csv(outfile, index = False)\n",
        "print(\"Written to csv file {}\".format(outfile))"
      ],
      "metadata": {
        "id": "fFabNVtqaEup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed5449d6-763d-4020-9a68-b5fb6253c51e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Written to csv file submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !kaggle competitions submit -c csci-ua-473-intro-to-machine-learning-fall22 -f submission.csv -m \"Training complete in 30m 15s; Best val loss: 0.00320\""
      ],
      "metadata": {
        "id": "KuJUPPCOd-MO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23e3e8c1-d0eb-4d9f-a08c-13350f0036ae"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 122k/122k [00:02<00:00, 56.2kB/s]\n",
            "Successfully submitted to CSCI-UA. 473 Intro to Machine Learning, Fall 2022"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "wUG7bK_rZwqd"
      }
    }
  ]
}